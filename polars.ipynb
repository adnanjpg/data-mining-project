{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "- find null values\n",
    "- replace with feature mean\n",
    "- find outliers (especially m2)\n",
    "- enumarate categorical features\n",
    "- drop title col\n",
    "- drop id col\n",
    "- convert all prices to try\n",
    "- drop lat lon\n",
    "- convert date values to be of the same race\n",
    "- drop type (bcz all values are flat)\n",
    "- drop currency\n",
    "- remove outlier prices (25000 TL, 8500000TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"real_estate_data.csv\", null_values=[\"Unknown\", \"None\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tmp  | Tmp | \n",
    "| ---  | --- | \n",
    "| TITLE      | title of the ad |\n",
    "| ID         | identification number of the ad |\n",
    "| PRICE      | price of the flat |\n",
    "| CURRENCY   | currency of the price     |\n",
    "| LOCCITY    | city of the building |\n",
    "| LOCOUNTY   | county of the building |\n",
    "| LOCDIST    | distirct of the building |\n",
    "| LAT        | latitude of the location of the building |\n",
    "| LON        | longitude of the location of the building |\n",
    "| DATE      | ad release date |\n",
    "| TYPE      | type of the ad |\n",
    "| M2        | size of the flat in meters |\n",
    "| ROOMS     | Rooms in flat |\n",
    "| AGE       | age of the building |\n",
    "| FLOOR     | floor number of the flat |\n",
    "| TFLOOR    | number of floors in building |\n",
    "| HEAT      | heating type of the building |\n",
    "| BATH      | number of bathrooms in the building |\n",
    "| FURN      | flat is furnitured or not |\n",
    "| STATUS    | occupied by owner, lessee or empty |\n",
    "| RESID     | building is in residence or not |\n",
    "| DUE       | monthly dues of the building |\n",
    "| LOAN      | flat is available for loan or not |\n",
    "| SALER     | saler of the flat is owner, real estate office or construction company |\n",
    "| EXC       | exchange is possible or not |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rank(feature: str, df: pl.DataFrame) -> pl.DataFrame:\n",
    "    new_name = f\"{feature} rank\"\n",
    "    if new_name not in df.columns:\n",
    "        return df.with_columns(pl.col(feature).rank(\"dense\").alias(new_name))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate features ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnecessary Features lat - lon - type - title - id - due and Status features from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df.select(pl.col(\"*\").exclude(\"lat\", \"lon\", \"type\", \"title\", \"Id\", \"due\",\"status\", ))\n",
    "df_updated.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all loc * to location column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"loc city\" in df_updated.columns:\n",
    "    df_updated = df_updated.select(pl.struct([\"loc city\", \"loc county\", \"loc dist\"]).map_elements(lambda x: f'{x[\"loc city\"]}-{x[\"loc county\"]}-{x[\"loc dist\"]}').alias(\"location\"), pl.exclude([\"loc city\", \"loc county\", \"loc dist\"]))\n",
    "df_updated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.group_by(\"location\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Eliminate currency feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.group_by(\"currency\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.01.2017 Günü Saat 15:30'da Belirlenen Gösterge Niteliğindeki Türkiye Cumhuriyet Merkez Bankası Kurları\n",
    "# https://www.tcmb.gov.tr/kurlar/kurlar_tr.html\n",
    "\n",
    "dolar_buy = 3.5338\n",
    "dolar_sell = 3.5402\n",
    "dolar = (dolar_buy + dolar_sell) / 2\n",
    "\n",
    "euro_buy = 3.7086\n",
    "euro_sell = 3.7153\n",
    "euro = (euro_buy + euro_sell) / 2\n",
    "\n",
    "brit_buy = 4.3488\n",
    "brit_sell = 4.3715\n",
    "brit = (brit_buy + brit_sell) / 2\n",
    "\n",
    "\n",
    "currency_dict = {\"Euro\": euro, \"US Dollar\": dolar, \"British Pound\": brit, \"Turkish Lira\": 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update currency column\n",
    "if \"currency\" in df_updated.columns: \n",
    "    df_updated = df_updated.select(pl.struct([\"currency\", \"price\"]).map_elements(lambda x: currency_dict[x[\"currency\"]] * x[\"price\"]).alias(\"price_tr\"), pl.col(\"*\").exclude(\"currency\", \"price\"))\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update dates with timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aylar = {\"Ocak\": \"01\", \"Şubat\": \"02\", \"Mart\": \"03\", \"Nisan\": \"04\", \"Mayıs\": \"05\", \"Haziran\": \"06\",\n",
    "             \"Temmuz\": \"07\", \"Ağustos\": \"08\", \"Eylül\": \"09\", \"Ekim\": \"10\", \"Kasım\": \"11\", \"Aralık\": \"12\"}\n",
    "\n",
    "def transform_date(date):\n",
    "    match date:\n",
    "        case str():\n",
    "            result = '-'.join(date.split()[::-1])\n",
    "            for ay, ay_kodu in aylar.items():\n",
    "                result = result.replace(ay, ay_kodu)\n",
    "            \n",
    "            return result\n",
    "        case _:\n",
    "            return date\n",
    "\n",
    "def transform_date_to_ms(date_str) -> int:\n",
    "    from dateutil import parser\n",
    "\n",
    "    match date_str:\n",
    "        case str():\n",
    "            return parser.parse(date_str, dayfirst=True).timestamp().__floor__()\n",
    "        case _:\n",
    "            return int(date_str)\n",
    "\n",
    "df_updated = df_updated.with_columns(pl.col(\"date\").map_elements(transform_date).map_elements(transform_date_to_ms))\n",
    "df_updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bath transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumeration Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_type, column in zip(df_updated.dtypes, df_updated.columns):\n",
    "    if d_type == pl.String:\n",
    "        df_updated = set_rank(column, df_updated)\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_updated.drop_nulls().select(pl.exclude(pl.String)).corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Room has  0.57 correlation with bath so we can fill null rooms using bath feature\n",
    "\n",
    "Heat has 0.077738 correlation with tfloor,  0.04684 corr with floor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df_updated.with_columns(pl.col(\"rooms rank\").map_elements(lambda s: s.fill_null(s.mode())).over(\"bath rank\")) \n",
    "df_updated = df_updated.with_columns(pl.col(\"heat rank\").map_elements(lambda s: s.fill_null(s.mode())).over(\"tfloor rank\")) \n",
    "df_updated = df_updated.with_columns(pl.col(\"bath rank\").map_elements(lambda s: s.fill_null(s.mode())).over(\"rooms rank\")) \n",
    "df_updated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df_updated.select(pl.exclude(\"furn rank\", \"resid rank\", \"loan rank\", \"furn\", \"loan\"))\n",
    "df_updated.select(pl.exclude(pl.String)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \"price_tr\"\n",
    "\n",
    "Q1 = pl.col(l).quantile(0.25)\n",
    "Q3 = pl.col(l).quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "LOWER = Q1 - 1.5 * IQR\n",
    "UPPER = Q3 + 1.5 * IQR\n",
    "\n",
    "df_updated = df_updated.filter((pl.col(l) > LOWER) & (pl.col(l) < UPPER))\n",
    "df_updated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_model = df_updated.select(pl.exclude(pl.String))\n",
    "\n",
    "# Separate target from predictors\n",
    "y = df_model.select(\"price_tr\").to_pandas()\n",
    "X = df_model.select(pl.exclude(\"price_tr\")).to_pandas()\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=42)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples in X_train:\", len(X_train))\n",
    "print(\"Number of samples in y_train:\", len(y_train))\n",
    "print(\"Number of samples in X_test:\", len(X_test))\n",
    "print(\"Number of samples in y_valid:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A general function that prints all important metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def print_model_stats(y_train_pred, y_test_pred, model):\n",
    "    # Calculate regression metrics for training set\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    rmse_train = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Calculate regression metrics for test set\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    train_accuracy = model.score(X_train, y_train) \n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Metrics for Training Set:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_train}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse_train}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae_train}\")\n",
    "    print(f\"R-squared (R2): {r2_train}\")\n",
    "    print(f\"Accuracy: {train_accuracy}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Metrics for Test Set:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse_test}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse_test}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae_test}\")\n",
    "    print(f\"R-squared (R2): {r2_test}\")\n",
    "    print(f\"Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print_model_stats(y_train_pred, y_test_pred, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "print_model_stats(y_train_pred, y_test_pred, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=tree, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "# Perform grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}\\n')\n",
    "\n",
    "# Create a new decision tree regressor with the best hyperparameters\n",
    "best_tree = DecisionTreeRegressor(max_depth=15, min_samples_leaf=4, min_samples_split=10)\n",
    "\n",
    "\n",
    "# Train the model on the entire training set\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_train_pred = best_tree.predict(X_train)\n",
    "y_test_pred = best_tree.predict(X_test)\n",
    "\n",
    "print_model_stats(y_train_pred, y_test_pred, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regression model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print_model_stats(y_train_pred, y_test_pred, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (optional but often recommended for RandomForest)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "print_model_stats(y_train_pred, y_test_pred, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for Training Set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train, y_train_pred, color='blue', label='Actual vs. Predicted (Training Set)')\n",
    "plt.title('Actual vs. Predicted Values - Training Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Test Set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred, color='red', label='Actual vs. Predicted (Test Set)')\n",
    "plt.title('Actual vs. Predicted Values - Test Set')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
